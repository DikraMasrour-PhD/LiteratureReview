---
tags:
  - Technique
  - DNN
---
#### Motivation
Skipping neurons in FC layers scheme relies on the belief that in FC layers different neuron units are responsible for different features, and therefore not all neuron units need to be activated for each input sample.
#### Auxiliary branches
- [[Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation]]
- [[Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning]]
- [[Conditional Computation in Neural Networks for Faster Models]]
#### Low-rank approximation
[[Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks|Low-Rank Approx]]
